{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c091922-869f-4648-bebc-bdc0636e74cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.data.replay_buffers import TensorDictReplayBuffer\n",
    "from torchrl.objectives import DQNLoss\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "from torchrl.modules import EGreedyModule, MLP, QValueModule\n",
    "from torchrl.data import OneHot\n",
    "from torchrl.data import DiscreteTensorSpec\n",
    "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
    "from torch.optim import Adam\n",
    "from torchrl.objectives import DQNLoss, SoftUpdate\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data import LazyTensorStorage, ReplayBuffer\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from torchrl.objectives.utils import ValueEstimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca3319e8-666b-4b06-a243-caaad17688ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustDQNLoss(DQNLoss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        value_network,\n",
    "        action_space,\n",
    "        delay_value=False,\n",
    "        reduction=\"mean\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor with corrected parameter signature, ensuring compatibility with DQNLoss.\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            value_network=value_network,\n",
    "            delay_value=delay_value,\n",
    "            reduction=reduction\n",
    "        )\n",
    "\n",
    "        # Corrected value estimator setup using TD(0)\n",
    "        self.make_value_estimator(ValueEstimators.TD0, gamma=0.99)\n",
    "\n",
    "        # Store keys separately instead of passing to super().__init__()\n",
    "        self.action_key = \"action\"\n",
    "        self.reward_key = \"reward\"\n",
    "        self.done_key = \"done\"\n",
    "        self.terminated_key = \"terminated\"\n",
    "        self.target_value_key = None\n",
    "\n",
    "\n",
    "    def _ensure_key_structure(self, tensordict: TensorDict) -> None:\n",
    "        \"\"\"\n",
    "        Ensures all required keys exist in the tensor dictionary.\n",
    "        \"\"\"\n",
    "        if \"next\" not in tensordict:\n",
    "            tensordict[\"next\"] = TensorDict({}, batch_size=tensordict.batch_size)\n",
    "\n",
    "        for key in [self.action_key, self.reward_key, self.done_key]:\n",
    "            if key not in tensordict:\n",
    "                print(f\"Warning: Missing '{key}' key. Initializing with zeros.\")\n",
    "                tensordict[key] = torch.zeros(\n",
    "                    tensordict.batch_size, dtype=torch.float32, device=self._get_safe_device(tensordict)\n",
    "                )\n",
    "\n",
    "        if self.done_key not in tensordict[\"next\"]:\n",
    "            tensordict[\"next\", self.done_key] = torch.zeros(\n",
    "                tensordict.batch_size, dtype=torch.bool, device=self._get_safe_device(tensordict)\n",
    "            )\n",
    "\n",
    "        if self.terminated_key not in tensordict[\"next\"]:\n",
    "            tensordict[\"next\", self.terminated_key] = tensordict[\"next\", self.done_key].clone()\n",
    "\n",
    "    def _get_safe_device(self, tensordict: TensorDict) -> torch.device:\n",
    "        \"\"\"\n",
    "        Determine a safe device to use based on tensor dictionary contents.\n",
    "        \"\"\"\n",
    "        for key in tensordict.keys():\n",
    "            value = tensordict.get(key)\n",
    "            if torch.is_tensor(value) and hasattr(value, 'device'):\n",
    "                return value.device\n",
    "        return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d09e743c-f0dc-411b-a4cd-fd5b4fe69f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_all_seeds(seed=42):\n",
    "    \"\"\"\n",
    "    Set all random seeds for deterministic results.\n",
    "    \n",
    "    Args:\n",
    "        seed: Seed value for random number generators\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c8e4698-b134-4d2c-a09e-6c43f3de116d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realistic Synthetic Data Generator\n",
    "def generate_synthetic_data(num_samples=1000, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic digital advertising data with realistic correlations and distributions.\n",
    "    Uses real-world inspired parameters and relationships between metrics.\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of data points to generate\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: Generated synthetic advertising data\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Generate realistic keyword data with distributions that match digital advertising metrics\n",
    "    data = {}\n",
    "    \n",
    "    # Keywords with realistic thematic structure\n",
    "    keyword_categories = [\"shoes\", \"clothes\", \"electronics\", \"furniture\", \"beauty\", \"sports\", \"travel\", \"food\"]\n",
    "    modifier_list = [\"buy\", \"cheap\", \"best\", \"online\", \"discount\", \"premium\", \"compare\", \"review\"]\n",
    "    \n",
    "    keywords = []\n",
    "    for i in range(num_samples):\n",
    "        category = random.choice(keyword_categories)\n",
    "        modifier = random.choice(modifier_list) if random.random() > 0.5 else \"\"\n",
    "        specific = f\"item_{i % 50}\" if random.random() > 0.7 else \"\"\n",
    "        kw = f\"{modifier} {category} {specific}\".strip()\n",
    "        keywords.append(kw)\n",
    "    \n",
    "    data[\"keyword\"] = keywords\n",
    "    \n",
    "    # Competitiveness (follows a beta distribution as in real ad markets)\n",
    "    data[\"competitiveness\"] = np.random.beta(2, 3, num_samples)\n",
    "    \n",
    "    # Difficulty score correlates with competitiveness\n",
    "    base_difficulty = np.random.beta(2.5, 3.5, num_samples)\n",
    "    data[\"difficulty_score\"] = 0.7 * data[\"competitiveness\"] + 0.3 * base_difficulty\n",
    "    \n",
    "    # Organic rank (higher competitiveness leads to worse ranking)\n",
    "    data[\"organic_rank\"] = 1 + np.floor(9 * data[\"difficulty_score\"] + np.random.normal(0, 1, num_samples).clip(-2, 2))\n",
    "    data[\"organic_rank\"] = data[\"organic_rank\"].clip(1, 10).astype(int)\n",
    "    \n",
    "    # CTR follows a realistic distribution and correlates negatively with rank\n",
    "    base_ctr = np.random.beta(1.5, 10, num_samples)  # Realistic CTR distribution\n",
    "    rank_effect = (11 - data[\"organic_rank\"]) / 10  # Higher ranks have better CTR\n",
    "    data[\"organic_ctr\"] = (base_ctr * rank_effect * 0.3).clip(0.01, 0.3)\n",
    "    \n",
    "    # Organic clicks based on CTR and a base impression count\n",
    "    base_impressions = np.random.lognormal(8, 1, num_samples).astype(int)\n",
    "    data[\"organic_clicks\"] = (base_impressions * data[\"organic_ctr\"]).astype(int)\n",
    "    \n",
    "    # Paid CTR correlates with organic CTR but with more variance\n",
    "    data[\"paid_ctr\"] = (data[\"organic_ctr\"] * np.random.normal(1, 0.3, num_samples)).clip(0.01, 0.25)\n",
    "    \n",
    "    # Paid clicks\n",
    "    paid_impressions = np.random.lognormal(7, 1.2, num_samples).astype(int)\n",
    "    data[\"paid_clicks\"] = (paid_impressions * data[\"paid_ctr\"]).astype(int)\n",
    "    \n",
    "    # Cost per click higher for more competitive keywords\n",
    "    data[\"cost_per_click\"] = (0.5 + 9.5 * data[\"competitiveness\"] * np.random.normal(1, 0.2, num_samples)).clip(0.1, 10)\n",
    "    \n",
    "    # Ad spend based on CPC and clicks\n",
    "    data[\"ad_spend\"] = data[\"paid_clicks\"] * data[\"cost_per_click\"]\n",
    "    \n",
    "    # Conversion rate with realistic e-commerce distribution\n",
    "    data[\"conversion_rate\"] = np.random.beta(1.2, 15, num_samples).clip(0.01, 0.3)\n",
    "    \n",
    "    # Ad conversions\n",
    "    data[\"ad_conversions\"] = (data[\"paid_clicks\"] * data[\"conversion_rate\"]).astype(int)\n",
    "    \n",
    "    # Conversion value with variance\n",
    "    base_value = np.random.lognormal(4, 1, num_samples)\n",
    "    data[\"conversion_value\"] = data[\"ad_conversions\"] * base_value\n",
    "    \n",
    "    # Cost per acquisition\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        data[\"cost_per_acquisition\"] = np.where(\n",
    "            data[\"ad_conversions\"] > 0, \n",
    "            data[\"ad_spend\"] / data[\"ad_conversions\"], \n",
    "            500  # Default high CPA for no conversions\n",
    "        ).clip(5, 500)\n",
    "    \n",
    "    # ROAS (Return on Ad Spend)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        data[\"ad_roas\"] = np.where(\n",
    "            data[\"ad_spend\"] > 0,\n",
    "            data[\"conversion_value\"] / data[\"ad_spend\"],\n",
    "            0\n",
    "        ).clip(0.5, 5)\n",
    "    \n",
    "    # Impression share (competitive keywords have lower share)\n",
    "    data[\"impression_share\"] = (1 - 0.6 * data[\"competitiveness\"] * np.random.normal(1, 0.2, num_samples)).clip(0.1, 1.0)\n",
    "    \n",
    "    # Previous recommendation (binary: increase bid or decrease bid)\n",
    "    data[\"previous_recommendation\"] = np.random.choice([0, 1], size=num_samples)\n",
    "    \n",
    "    # Create DataFrame with all metrics\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117bb942-4a92-481f-bdb2-1b6d63b93d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the robust TorchRL Environment\n",
    "class AdOptimizationEnv(EnvBase):\n",
    "    \"\"\"\n",
    "    Environment for digital advertising bid optimization using reinforcement learning.\n",
    "    \n",
    "    The environment simulates the decision-making process for ad bid adjustments\n",
    "    based on various advertising metrics and performance indicators.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class-level declaration for immutable properties\n",
    "    _has_dynamic_specs = False\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initialize the environment with a dataset of advertising metrics.\n",
    "        \n",
    "        Args:\n",
    "            dataset: pandas.DataFrame containing advertising metrics\n",
    "        \"\"\"\n",
    "        # Initialize base class before any protected attribute access\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store dataset and define feature space\n",
    "        self.dataset = dataset\n",
    "        self.feature_columns = [\n",
    "            \"competitiveness\", \"difficulty_score\", \"organic_rank\", \n",
    "            \"organic_clicks\", \"organic_ctr\", \"paid_clicks\", \"paid_ctr\", \n",
    "            \"ad_spend\", \"ad_conversions\", \"ad_roas\", \"conversion_rate\", \n",
    "            \"cost_per_click\", \"cost_per_acquisition\", \"impression_share\"\n",
    "        ]\n",
    "        self.num_features = len(self.feature_columns)\n",
    "        \n",
    "        # Define action space: binary decision (decrease/increase bid)\n",
    "        self.action_spec = OneHot(n=2, dtype=torch.int64)\n",
    "        \n",
    "        # Initialize episode state variables\n",
    "        self.episode_step = 0\n",
    "        self.max_episode_steps = 100\n",
    "        self.current_sample = None\n",
    "        self.rng = None\n",
    "        \n",
    "        # Initialize random generator\n",
    "        self._set_seed(None)\n",
    "        \n",
    "        # Reset environment to initialize observation space\n",
    "        self.reset()\n",
    "\n",
    "    def _reset(self, tensordict=None):\n",
    "        \"\"\"\n",
    "        Reset the environment to an initial state.\n",
    "        \"\"\"\n",
    "        sample = self.dataset.sample(1)\n",
    "        state = torch.tensor(sample[self.feature_columns].values, dtype=torch.float32).squeeze()\n",
    "        \n",
    "        # Ensure observation is set\n",
    "        tensordict = TensorDict({\n",
    "            \"observation\": state,  # Ensure observation key exists\n",
    "            \"step_count\": torch.tensor(self.episode_step, dtype=torch.int64),\n",
    "            \"reward\": torch.tensor(0.0, dtype=torch.float32),  # Initial reward is zero\n",
    "        }, batch_size=[])\n",
    "\n",
    "        return tensordict\n",
    "\n",
    "    def _step(self, tensordict):\n",
    "        \"\"\"\n",
    "        Execute one step in the environment.\n",
    "        \"\"\"\n",
    "        action = tensordict[\"action\"].argmax(dim=-1).item() if \"action\" in tensordict else 0\n",
    "        \n",
    "        next_sample = self.dataset.sample(1)\n",
    "        next_state = torch.tensor(next_sample[self.feature_columns].values, dtype=torch.float32).squeeze()\n",
    "        \n",
    "        reward = self._compute_reward(action, next_sample)\n",
    "        \n",
    "        self.episode_step += 1\n",
    "        done = self.episode_step >= self.max_episode_steps\n",
    "        \n",
    "        step_td = TensorDict({\n",
    "            \"observation\": next_state,  # Ensure observation key exists\n",
    "            \"reward\": torch.tensor(reward, dtype=torch.float32),\n",
    "            \"done\": torch.tensor(done, dtype=torch.bool),\n",
    "            \"terminated\": torch.tensor(done, dtype=torch.bool),\n",
    "            \"step_count\": torch.tensor(self.episode_step, dtype=torch.int64),\n",
    "            \"next\": TensorDict({\n",
    "                \"observation\": next_state.clone(),  # Ensure next observation exists\n",
    "                \"reward\": torch.tensor(reward, dtype=torch.float32),\n",
    "                \"done\": torch.tensor(done, dtype=torch.bool),\n",
    "                \"terminated\": torch.tensor(done, dtype=torch.bool),\n",
    "            }, batch_size=[])\n",
    "        }, batch_size=[])\n",
    "\n",
    "        return step_td\n",
    "\n",
    "    \n",
    "    def _compute_reward(self, action, sample):\n",
    "        \"\"\"\n",
    "        Compute reward based on a nuanced digital advertising strategy.\n",
    "        \n",
    "        Args:\n",
    "            action: Binary action (0: decrease bid, 1: increase bid)\n",
    "            sample: Dataframe row with advertising metrics\n",
    "            \n",
    "        Returns:\n",
    "            float: Computed reward value\n",
    "        \"\"\"\n",
    "        # Extract relevant metrics\n",
    "        roas = sample[\"ad_roas\"].values[0]\n",
    "        cpa = sample[\"cost_per_acquisition\"].values[0]\n",
    "        ad_spend = sample[\"ad_spend\"].values[0]\n",
    "        ctr = sample[\"paid_ctr\"].values[0]\n",
    "        conv_rate = sample[\"conversion_rate\"].values[0]\n",
    "        \n",
    "        # Conservative action (decrease bid/budget)\n",
    "        if action == 0:\n",
    "            # Good when: high CPA, low ROAS, or acceptable performance with high spend\n",
    "            if cpa > 200 or roas < 1.2:\n",
    "                return 1.0  # Good decision to be conservative\n",
    "            elif ad_spend > 5000 and roas < 2.0:\n",
    "                return 0.5  # Somewhat good decision (high spend, moderate ROAS)\n",
    "            else:\n",
    "                return -1.0  # Bad decision (missed opportunity)\n",
    "        \n",
    "        # Aggressive action (increase bid/budget)\n",
    "        else:  # action == 1\n",
    "            # Good when: low CPA, high ROAS, high CTR, good conversion rate\n",
    "            if roas > 2.5 or (ctr > 0.15 and conv_rate > 0.1):\n",
    "                return 1.5  # Very good decision\n",
    "            elif roas > 1.5 and cpa < 100:\n",
    "                return 1.0  # Good decision\n",
    "            elif roas > 1.0:\n",
    "                return 0.2  # Acceptable decision\n",
    "            else:\n",
    "                return -1.5  # Very bad decision (wasting money)\n",
    "\n",
    "    def _set_seed(self, seed: Optional[int]) -> torch.Generator:\n",
    "        \"\"\"\n",
    "        Set random seed for environment reproducibility.\n",
    "        \n",
    "        Args:\n",
    "            seed: Random seed value\n",
    "            \n",
    "        Returns:\n",
    "            torch.Generator: Seeded random number generator\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            # Set all relevant random seeds\n",
    "            rng = torch.manual_seed(seed)\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            self.rng = torch.Generator().manual_seed(seed)\n",
    "            return self.rng\n",
    "        else:\n",
    "            self.rng = torch.Generator()\n",
    "            return self.rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06b8365b-80bb-421f-9090-423c94fbbb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ad_optimization_agent(dataset, num_iterations=500, batch_size=100, sample_size=128, log_interval=10):\n",
    "    \"\"\"\n",
    "    Train an RL agent for digital ad optimization with fixed tensor structures.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame with advertising metrics\n",
    "        num_iterations: Number of training iterations\n",
    "        batch_size: Number of environment interactions per batch\n",
    "        sample_size: Number of samples from replay buffer per update\n",
    "        log_interval: Interval for logging training progress\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_policy, training_metrics, environment)\n",
    "    \"\"\"\n",
    "    # Initialize Environment\n",
    "    env = AdOptimizationEnv(dataset)\n",
    "    state_dim = env.num_features\n",
    "    action_dim = env.action_spec.n\n",
    "    \n",
    "    # Create neural network value function\n",
    "    value_mlp = MLP(\n",
    "        in_features=state_dim, \n",
    "        out_features=action_dim, \n",
    "        num_cells=[128, 64],\n",
    "        activation_class=nn.ReLU\n",
    "    )\n",
    "    \n",
    "    # Explicit tensor key mapping for value network\n",
    "    value_net = TensorDictModule(\n",
    "        value_mlp, \n",
    "        in_keys=[\"observation\"], \n",
    "        out_keys=[\"action_value\"]\n",
    "    )\n",
    "    \n",
    "    # Create Q-value module with explicit key configuration\n",
    "    # Note: QValueModule requires careful configuration of out_keys parameter\n",
    "    # The error occurs because QValueModule tries to access out_keys[1] when it's not provided\n",
    "    policy = TensorDictSequential(\n",
    "        value_net,\n",
    "        QValueModule(\n",
    "            action_value_key=\"action_value\",\n",
    "            out_keys=[\"action\", \"action_value\", \"chosen_action_value\"],\n",
    "            spec=env.action_spec  # Ensures the policy has an action space\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Explicitly set spec to policy to avoid \"action_space\" retrieval error\n",
    "    policy.spec = env.action_spec  # Fixes the issue\n",
    "    \n",
    "    # Exploration strategy with epsilon annealing\n",
    "    exploration_module = EGreedyModule(\n",
    "        env.action_spec, \n",
    "        annealing_num_steps=10000, \n",
    "        eps_init=0.9, \n",
    "        eps_end=0.05,\n",
    "        action_key=\"action\"\n",
    "    )\n",
    "    \n",
    "    # Complete policy with exploration\n",
    "    policy_explore = TensorDictSequential(policy, exploration_module)\n",
    "    \n",
    "    # Data collection setup\n",
    "    init_rand_steps = 1000\n",
    "    frames_per_batch = batch_size\n",
    "    optim_steps = 5\n",
    "    \n",
    "    # Create data collector\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        policy_explore,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=-1,\n",
    "        init_random_frames=init_rand_steps,\n",
    "        device=\"cpu\",\n",
    "        storing_device=\"cpu\",\n",
    "        return_same_td=True,\n",
    "        split_trajs=False\n",
    "    )\n",
    "    \n",
    "    # Create replay buffer for experience replay\n",
    "    rb = ReplayBuffer(storage=LazyTensorStorage(max_size=20000))\n",
    "    \n",
    "    # Use DQNLoss with standard parameter interface\n",
    "    loss = RobustDQNLoss(\n",
    "        value_network=policy, \n",
    "        action_space=env.action_spec, \n",
    "        delay_value=True\n",
    "    )\n",
    "    \n",
    "    # Optimizer and target network updater\n",
    "    optim = Adam(loss.parameters(), lr=0.001)\n",
    "    updater = SoftUpdate(loss, eps=0.95)\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    total_frames = 0\n",
    "    rewards = []\n",
    "    avg_losses = []\n",
    "    epsilon_values = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for i, data in enumerate(collector):\n",
    "        if i >= num_iterations:\n",
    "            break\n",
    "            \n",
    "        # Validate and fix tensor dictionary structure\n",
    "        ensure_valid_td_structure(data)\n",
    "        \n",
    "        # Track rewards\n",
    "        if \"reward\" in data:\n",
    "            reward = data[\"reward\"].sum().item() / frames_per_batch\n",
    "            rewards.append(reward)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "        \n",
    "        # Track exploration rate\n",
    "        epsilon_values.append(exploration_module.eps.item())\n",
    "        \n",
    "        # Store in replay buffer - make a deep copy to avoid reference issues\n",
    "        rb.extend(data.clone())\n",
    "        \n",
    "        batch_losses = []\n",
    "        if len(rb) > init_rand_steps:\n",
    "            # Optimization steps\n",
    "            for _ in range(optim_steps):\n",
    "                # Sample from replay buffer\n",
    "                sample = rb.sample(sample_size)\n",
    "                \n",
    "                # Ensure proper structure in the sample\n",
    "                ensure_valid_td_structure(sample)\n",
    "                \n",
    "                # Compute loss and update\n",
    "                loss_vals = loss(sample)\n",
    "                \n",
    "                if \"loss\" in loss_vals:\n",
    "                    loss_val = loss_vals[\"loss\"].item()\n",
    "                    batch_losses.append(loss_val)\n",
    "                    \n",
    "                    loss_vals[\"loss\"].backward()\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "                else:\n",
    "                    print(\"Warning: Loss computation did not return 'loss' key\")\n",
    "                \n",
    "                # Update exploration and target network\n",
    "                exploration_module.step(frames_per_batch)\n",
    "                updater.step()\n",
    "            \n",
    "            if batch_losses:\n",
    "                avg_losses.append(np.mean(batch_losses))\n",
    "        \n",
    "        total_frames += frames_per_batch\n",
    "        \n",
    "        # Print progress\n",
    "        if i % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Iteration: {i}/{num_iterations}, Frames: {total_frames}, \"\n",
    "                  f\"Avg Reward: {np.mean(rewards[-10:]) if rewards else 0:.4f}, \"\n",
    "                  f\"Epsilon: {exploration_module.eps.item():.4f}, \"\n",
    "                  f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Create clean policy without exploration for evaluation\n",
    "    eval_policy = TensorDictSequential(value_net, QValueModule(spec=env.action_spec))\n",
    "    eval_policy.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    # Compile training metrics\n",
    "    training_metrics = {\n",
    "        \"rewards\": rewards,\n",
    "        \"losses\": avg_losses,\n",
    "        \"epsilon_values\": epsilon_values,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"training_time\": time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    return eval_policy, training_metrics, env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b7fdc4-98ad-4ee2-8688-ae796889a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_valid_td_structure(td):\n",
    "    \"\"\"\n",
    "    Ensure tensor dictionary has valid structure for DQN learning.\n",
    "    \"\"\"\n",
    "    # Ensure 'observation' key exists\n",
    "    if \"observation\" not in td:\n",
    "        print(\"Warning: Missing 'observation' key in tensor dictionary\")\n",
    "        td[\"observation\"] = torch.zeros(td.batch_size, dtype=torch.float32)\n",
    "        \n",
    "    # Ensure reward structure\n",
    "    if \"reward\" not in td:\n",
    "        if \"next\" in td and \"reward\" in td[\"next\"]:\n",
    "            td[\"reward\"] = td[\"next\", \"reward\"].clone()\n",
    "        else:\n",
    "            td[\"reward\"] = torch.zeros(td.batch_size, dtype=torch.float32)\n",
    "            \n",
    "    # Ensure 'next' exists\n",
    "    if \"next\" not in td:\n",
    "        td[\"next\"] = TensorDict({}, batch_size=td.batch_size)\n",
    "        \n",
    "    # Ensure 'next' contains 'observation'\n",
    "    if \"observation\" not in td[\"next\"]:\n",
    "        td[\"next\", \"observation\"] = torch.zeros(td.batch_size, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb69dd91-0803-4dec-bf55-cfd8768e6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(policy, env, num_episodes=100):\n",
    "    \"\"\"\n",
    "    Evaluate a trained policy without exploration.\n",
    "    \n",
    "    Args:\n",
    "        policy: Trained policy without exploration module\n",
    "        env: Environment to evaluate on\n",
    "        num_episodes: Number of episodes to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    episode_lengths = []\n",
    "    action_counts = {0: 0, 1: 0}  # 0: conservative, 1: aggressive\n",
    "    decisions = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        # Reset environment\n",
    "        tensordict = env.reset()\n",
    "        \n",
    "        while not done:\n",
    "            # Get action from policy\n",
    "            with torch.no_grad():\n",
    "                action_td = policy(tensordict)\n",
    "                action = action_td[\"action\"].argmax(dim=-1).item()\n",
    "            \n",
    "            # Record state and action\n",
    "            states.append(tensordict[\"observation\"].numpy())\n",
    "            action_counts[action] += 1\n",
    "            \n",
    "            # Step environment\n",
    "            next_td = env.step(action_td)\n",
    "            reward = next_td[\"reward\"].item()\n",
    "            done = next_td[\"done\"].item()\n",
    "            \n",
    "            # Update for next step\n",
    "            tensordict = next_td\n",
    "            \n",
    "            # Record results\n",
    "            decisions.append((action, reward))\n",
    "            rewards.append(reward)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "        \n",
    "        total_reward += episode_reward\n",
    "        episode_lengths.append(steps)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    avg_episode_length = np.mean(episode_lengths)\n",
    "    action_distribution = {k: v / sum(action_counts.values()) for k, v in action_counts.items()}\n",
    "    \n",
    "    # Calculate success rate (positive reward decisions)\n",
    "    correct_decisions = sum(1 for a, r in decisions if r > 0)\n",
    "    success_rate = correct_decisions / len(decisions) if decisions else 0\n",
    "    \n",
    "    # Calculate metrics by action type\n",
    "    conservative_rewards = [r for (a, r) in decisions if a == 0]\n",
    "    aggressive_rewards = [r for (a, r) in decisions if a == 1]\n",
    "    \n",
    "    avg_conservative_reward = np.mean(conservative_rewards) if conservative_rewards else 0\n",
    "    avg_aggressive_reward = np.mean(aggressive_rewards) if aggressive_rewards else 0\n",
    "    \n",
    "    # Compile all metrics\n",
    "    metrics = {\n",
    "        \"avg_reward\": avg_reward,\n",
    "        \"avg_episode_length\": avg_episode_length,\n",
    "        \"action_distribution\": action_distribution,\n",
    "        \"success_rate\": success_rate,\n",
    "        \"avg_conservative_reward\": avg_conservative_reward,\n",
    "        \"avg_aggressive_reward\": avg_aggressive_reward,\n",
    "        \"states\": np.array(states) if states else np.array([]),\n",
    "        \"decisions\": decisions,\n",
    "        \"rewards\": rewards\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd465f77-e22d-4afe-abe7-429577cf8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_evaluation(metrics, feature_columns, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create visualizations for evaluation metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary of evaluation metrics from evaluate_policy\n",
    "        feature_columns: List of feature column names\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Set visualization style\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    \n",
    "    # Create multi-panel figure\n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    fig.suptitle(\"Ad Optimization RL Agent Evaluation\", fontsize=16)\n",
    "    \n",
    "    # 1. Action Distribution\n",
    "    ax1 = fig.add_subplot(2, 3, 1)\n",
    "    actions = [\"Conservative\", \"Aggressive\"]\n",
    "    frequencies = list(metrics[\"action_distribution\"].values())\n",
    "    ax1.bar(actions, frequencies, color=[\"skyblue\", \"coral\"])\n",
    "    ax1.set_title(\"Action Distribution\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # 2. Reward by Action Type\n",
    "    ax2 = fig.add_subplot(2, 3, 2)\n",
    "    ax2.bar([\"Conservative\", \"Aggressive\"], \n",
    "            [metrics[\"avg_conservative_reward\"], metrics[\"avg_aggressive_reward\"]], \n",
    "            color=[\"skyblue\", \"coral\"])\n",
    "    ax2.set_title(\"Average Reward by Action Type\")\n",
    "    ax2.set_ylabel(\"Average Reward\")\n",
    "    \n",
    "    # 3. Feature Importance (using action correlation)\n",
    "    states = metrics[\"states\"]\n",
    "    decisions = np.array([a for a, _ in metrics[\"decisions\"]])\n",
    "    \n",
    "    # Calculate correlation between features and actions\n",
    "    correlations = []\n",
    "    for i in range(states.shape[1]):\n",
    "        if states.size > 0 and decisions.size > 0:\n",
    "            corr = np.corrcoef(states[:, i], decisions)[0, 1]\n",
    "            correlations.append(corr)\n",
    "        else:\n",
    "            correlations.append(0)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    ax3 = fig.add_subplot(2, 3, 3)\n",
    "    feature_importance = pd.Series(correlations, index=feature_columns)\n",
    "    feature_importance.sort_values(ascending=False).plot(kind='bar', ax=ax3)\n",
    "    ax3.set_title(\"Feature Correlation with Actions\")\n",
    "    ax3.set_ylabel(\"Correlation Coefficient\")\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    # 4. Reward Distribution\n",
    "    ax4 = fig.add_subplot(2, 3, 4)\n",
    "    sns.histplot(metrics[\"rewards\"], kde=True, ax=ax4)\n",
    "    ax4.set_title(\"Reward Distribution\")\n",
    "    ax4.set_xlabel(\"Reward\")\n",
    "    ax4.set_ylabel(\"Frequency\")\n",
    "    \n",
    "    # 5. Decision Quality Matrix (Confusion Matrix style)\n",
    "    ax5 = fig.add_subplot(2, 3, 5)\n",
    "    decision_quality = np.zeros((2, 2))\n",
    "    \n",
    "    for action, reward in metrics[\"decisions\"]:\n",
    "        quality = 1 if reward > 0 else 0\n",
    "        decision_quality[action, quality] += 1\n",
    "    \n",
    "    # Normalize by row to prevent division by zero\n",
    "    row_sums = decision_quality.sum(axis=1, keepdims=True)\n",
    "    # Handle cases where row sum is zero\n",
    "    row_sums = np.where(row_sums == 0, 1, row_sums)\n",
    "    decision_quality_norm = decision_quality / row_sums\n",
    "    \n",
    "    sns.heatmap(decision_quality_norm, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "                xticklabels=[\"Poor\", \"Good\"], \n",
    "                yticklabels=[\"Conservative\", \"Aggressive\"],\n",
    "                ax=ax5)\n",
    "    ax5.set_title(\"Decision Quality Matrix\")\n",
    "    ax5.set_ylabel(\"Action\")\n",
    "    ax5.set_xlabel(\"Decision Quality\")\n",
    "    \n",
    "    # 6. Feature Distribution by Action\n",
    "    ax6 = fig.add_subplot(2, 3, 6)\n",
    "    \n",
    "    # Proceed only if we have sufficient data\n",
    "    if states.size > 0 and decisions.size > 0:\n",
    "        # Choose the most important feature based on correlation\n",
    "        important_feature_idx = np.abs(correlations).argmax()\n",
    "        important_feature_name = feature_columns[important_feature_idx]\n",
    "        \n",
    "        # Split the feature values by action\n",
    "        feature_by_action = {\n",
    "            \"Conservative\": states[decisions == 0, important_feature_idx],\n",
    "            \"Aggressive\": states[decisions == 1, important_feature_idx]\n",
    "        }\n",
    "        \n",
    "        # Plot only if both actions have some data\n",
    "        if len(feature_by_action[\"Conservative\"]) > 0 and len(feature_by_action[\"Aggressive\"]) > 0:\n",
    "            sns.kdeplot(data=feature_by_action, common_norm=False, ax=ax6)\n",
    "            ax6.set_title(f\"Distribution of {important_feature_name} by Action\")\n",
    "            ax6.set_xlabel(important_feature_name)\n",
    "            ax6.set_ylabel(\"Density\")\n",
    "        else:\n",
    "            ax6.text(0.5, 0.5, \"Insufficient data for KDE plot\", \n",
    "                    horizontalalignment='center', verticalalignment='center')\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, \"No data available for feature distribution\", \n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f\"{output_dir}/agent_evaluation.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07ccb105-07d9-4ea6-b3ea-793f76994eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_progress(training_metrics, output_dir=\"plots\", window_size=20):\n",
    "    \"\"\"\n",
    "    Visualize the training progress based on collected metrics.\n",
    "    \n",
    "    Args:\n",
    "        training_metrics: Dictionary of training metrics\n",
    "        output_dir: Directory to save plots\n",
    "        window_size: Window size for smoothing\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rewards = training_metrics[\"rewards\"]\n",
    "    losses = training_metrics[\"losses\"]\n",
    "    epsilons = training_metrics[\"epsilon_values\"]\n",
    "    \n",
    "    # Create figure with 3 subplots\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 15), sharex=True)\n",
    "    fig.suptitle(\"RL Training Progress\", fontsize=16)\n",
    "    \n",
    "    # 1. Plot rewards\n",
    "    axes[0].plot(rewards, alpha=0.3, color='blue', label=\"Episode Rewards\")\n",
    "    \n",
    "    # Add smoothed rewards\n",
    "    if len(rewards) >= window_size:\n",
    "        smoothed_rewards = []\n",
    "        for i in range(len(rewards) - window_size + 1):\n",
    "            smoothed_rewards.append(np.mean(rewards[i:i+window_size]))\n",
    "        axes[0].plot(range(window_size-1, len(rewards)), smoothed_rewards, \n",
    "                   color='red', linewidth=2, label=f\"Moving Average ({window_size})\")\n",
    "    \n",
    "    axes[0].set_ylabel(\"Average Reward\")\n",
    "    axes[0].set_title(\"Training Rewards\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Plot losses\n",
    "    if losses:\n",
    "        axes[1].plot(losses, color='purple', alpha=0.5, label=\"Training Loss\")\n",
    "        \n",
    "        # Add smoothed losses\n",
    "        if len(losses) >= window_size:\n",
    "            smoothed_losses = []\n",
    "            for i in range(len(losses) - window_size + 1):\n",
    "                smoothed_losses.append(np.mean(losses[i:i+window_size]))\n",
    "            axes[1].plot(range(window_size-1, len(losses)), smoothed_losses, \n",
    "                       color='darkred', linewidth=2, label=f\"Moving Average ({window_size})\")\n",
    "        \n",
    "        axes[1].set_ylabel(\"Loss\")\n",
    "        axes[1].set_title(\"Training Loss\")\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Plot exploration rate\n",
    "    axes[2].plot(epsilons, color='green', label=\"Exploration Rate (ε)\")\n",
    "    axes[2].set_ylim(0, 1)\n",
    "    axes[2].set_xlabel(\"Episodes\")\n",
    "    axes[2].set_ylabel(\"Epsilon (ε)\")\n",
    "    axes[2].set_title(\"Exploration Rate\")\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/training_progress.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce3eb223-7bec-404a-ad5c-5d937181f061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ad_strategy(policy, dataset, feature_columns, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Analyze the learned advertising strategy for different input scenarios.\n",
    "    \n",
    "    Args:\n",
    "        policy: The trained policy without exploration module\n",
    "        dataset: The dataset containing ad metrics\n",
    "        feature_columns: List of feature column names\n",
    "        output_dir: Directory to save plots\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with strategy analysis for different scenarios\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Prepare scenarios\n",
    "    scenarios = [\n",
    "        {\"name\": \"High ROAS, Low CPA\", \n",
    "         \"filters\": {\"ad_roas\": lambda x: x > 3.0, \"cost_per_acquisition\": lambda x: x < 50}},\n",
    "        {\"name\": \"Low ROAS, High CPA\", \n",
    "         \"filters\": {\"ad_roas\": lambda x: x < 1.0, \"cost_per_acquisition\": lambda x: x > 200}},\n",
    "        {\"name\": \"High CTR\", \n",
    "         \"filters\": {\"paid_ctr\": lambda x: x > 0.15}},\n",
    "        {\"name\": \"Low CTR\", \n",
    "         \"filters\": {\"paid_ctr\": lambda x: x < 0.05}},\n",
    "        {\"name\": \"High Ad Spend\", \n",
    "         \"filters\": {\"ad_spend\": lambda x: x > 5000}},\n",
    "        {\"name\": \"Low Ad Spend\", \n",
    "         \"filters\": {\"ad_spend\": lambda x: x < 500}},\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # Filter dataset\n",
    "        filtered_df = dataset.copy()\n",
    "        for col, condition in scenario[\"filters\"].items():\n",
    "            filtered_df = filtered_df[condition(filtered_df[col])]\n",
    "        \n",
    "        if len(filtered_df) < 10:\n",
    "            print(f\"Not enough data for scenario: {scenario['name']}\")\n",
    "            continue\n",
    "        \n",
    "        # Sample from filtered dataset\n",
    "        samples = filtered_df.sample(min(100, len(filtered_df)))\n",
    "        \n",
    "        # Get policy decisions\n",
    "        actions = []\n",
    "        for _, row in samples.iterrows():\n",
    "            state = torch.tensor(row[feature_columns].values, dtype=torch.float32)\n",
    "            tensordict = TensorDict({\"observation\": state}, batch_size=[])\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                action_td = policy(tensordict)\n",
    "                action = action_td[\"action\"].argmax(dim=-1).item()\n",
    "            actions.append(action)\n",
    "        \n",
    "        # Calculate action distribution\n",
    "        conservative_ratio = actions.count(0) / len(actions) if actions else 0\n",
    "        aggressive_ratio = actions.count(1) / len(actions) if actions else 0\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"scenario\": scenario[\"name\"],\n",
    "            \"sample_size\": len(samples),\n",
    "            \"conservative_ratio\": conservative_ratio,\n",
    "            \"aggressive_ratio\": aggressive_ratio\n",
    "        })\n",
    "    \n",
    "    # Visualize scenario analysis if we have results\n",
    "    if results:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        scenarios = [r[\"scenario\"] for r in results]\n",
    "        conservative = [r[\"conservative_ratio\"] * 100 for r in results]\n",
    "        aggressive = [r[\"aggressive_ratio\"] * 100 for r in results]\n",
    "        \n",
    "        x = range(len(scenarios))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar([i - width/2 for i in x], conservative, width, label='Conservative (Decrease)', color='skyblue')\n",
    "        plt.bar([i + width/2 for i in x], aggressive, width, label='Aggressive (Increase)', color='coral')\n",
    "        \n",
    "        plt.xlabel('Scenario')\n",
    "        plt.ylabel('Action Distribution (%)')\n",
    "        plt.title('Ad Strategy by Scenario')\n",
    "        plt.xticks(x, scenarios, rotation=45, ha='right')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/strategy_analysis.png\", dpi=300, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Return the results\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36bc8c1f-f45a-4468-aa15-a4496007cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_analysis(policy, env, feature_columns, num_samples=1000, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Analyze feature importance using perturbation analysis.\n",
    "    \n",
    "    Args:\n",
    "        policy: The trained policy\n",
    "        env: The environment\n",
    "        feature_columns: List of feature column names\n",
    "        num_samples: Number of samples to use for analysis\n",
    "        output_dir: Directory to save plots\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with feature importance scores\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate samples\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        tensordict = env.reset()\n",
    "        if \"observation\" in tensordict:\n",
    "            samples.append(tensordict[\"observation\"].clone())\n",
    "    \n",
    "    if not samples:\n",
    "        print(\"Warning: No valid samples collected for feature importance analysis\")\n",
    "        return pd.DataFrame({\"Feature\": feature_columns, \"Importance\": [0] * len(feature_columns)})\n",
    "    \n",
    "    samples = torch.stack(samples)\n",
    "    \n",
    "    # Calculate baseline predictions\n",
    "    with torch.no_grad():\n",
    "        baseline_actions = []\n",
    "        for i in range(num_samples):\n",
    "            tensordict = TensorDict({\"observation\": samples[i]}, batch_size=[])\n",
    "            action_td = policy(tensordict)\n",
    "            action = action_td[\"action\"].argmax(dim=-1).item()\n",
    "            baseline_actions.append(action)\n",
    "    \n",
    "    # Calculate importance for each feature\n",
    "    importance_scores = []\n",
    "    \n",
    "    for feature_idx in range(samples.shape[1]):\n",
    "        # Create perturbed samples\n",
    "        perturbed_samples = samples.clone()\n",
    "        \n",
    "        # Permute the feature\n",
    "        permutation = torch.randperm(num_samples)\n",
    "        perturbed_samples[:, feature_idx] = perturbed_samples[permutation, feature_idx]\n",
    "        \n",
    "        # Get predictions on perturbed data\n",
    "        perturbed_actions = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(num_samples):\n",
    "                tensordict = TensorDict({\"observation\": perturbed_samples[i]}, batch_size=[])\n",
    "                action_td = policy(tensordict)\n",
    "                action = action_td[\"action\"].argmax(dim=-1).item()\n",
    "                perturbed_actions.append(action)\n",
    "        \n",
    "        # Calculate importance score (change in predictions)\n",
    "        disagreement = sum(b != p for b, p in zip(baseline_actions, perturbed_actions))\n",
    "        importance = disagreement / num_samples\n",
    "        \n",
    "        importance_scores.append(importance)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_columns,\n",
    "        'Importance': importance_scores\n",
    "    })\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title('Feature Importance Analysis')\n",
    "    plt.xlabel('Importance Score (Higher = More Important)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/feature_importance.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    return importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c191c0e-ae12-4128-b6fc-9becda7eb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def run_complete_ad_optimization_pipeline(num_samples=5000, \n",
    "                                         train_iterations=300, \n",
    "                                         eval_episodes=100,\n",
    "                                         output_dir=\"ad_optimization_results\"):\n",
    "    \"\"\"\n",
    "    Run the complete digital advertising optimization pipeline:\n",
    "    1. Generate synthetic data\n",
    "    2. Train RL agent\n",
    "    3. Evaluate agent\n",
    "    4. Analyze strategy and feature importance\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of synthetic data samples to generate\n",
    "        train_iterations: Number of training iterations\n",
    "        eval_episodes: Number of episodes for evaluation\n",
    "        output_dir: Directory to save results and plots\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all results and models\n",
    "    \"\"\"\n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = f\"{output_dir}_{timestamp}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    plot_dir = f\"{run_dir}/plots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting digital advertising optimization pipeline...\")\n",
    "    print(f\"Results will be saved to: {run_dir}\")\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    set_all_seeds(42)\n",
    "    \n",
    "    # 1. Generate synthetic data\n",
    "    print(f\"Generating synthetic dataset with {num_samples} samples...\")\n",
    "    dataset = generate_synthetic_data(num_samples)\n",
    "    \n",
    "    # Save dataset\n",
    "    dataset.to_csv(f\"{run_dir}/synthetic_ad_data.csv\", index=False)\n",
    "    print(f\"Synthetic dataset saved to {run_dir}/synthetic_ad_data.csv\")\n",
    "    \n",
    "    # 2. Train RL agent\n",
    "    print(f\"Training RL agent for {train_iterations} iterations...\")\n",
    "    policy, training_metrics, env = train_ad_optimization_agent(\n",
    "        dataset, \n",
    "        num_iterations=train_iterations, \n",
    "        batch_size=100, \n",
    "        sample_size=128\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b13b2-4193-4658-a00f-10dada2ed7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8765be6-ea7c-42c9-a3a1-1d6624af89e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution function\n",
    "def run_complete_ad_optimization_pipeline(num_samples=5000, \n",
    "                                         train_iterations=300, \n",
    "                                         eval_episodes=100,\n",
    "                                         output_dir=\"ad_optimization_results\"):\n",
    "    \"\"\"\n",
    "    Run the complete digital advertising optimization pipeline:\n",
    "    1. Generate synthetic data\n",
    "    2. Train RL agent\n",
    "    3. Evaluate agent\n",
    "    4. Analyze strategy and feature importance\n",
    "    \n",
    "    Args:\n",
    "        num_samples: Number of synthetic data samples to generate\n",
    "        train_iterations: Number of training iterations\n",
    "        eval_episodes: Number of episodes for evaluation\n",
    "        output_dir: Directory to save results and plots\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing all results and models\n",
    "    \"\"\"\n",
    "    # Create output directory with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = f\"{output_dir}_{timestamp}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    plot_dir = f\"{run_dir}/plots\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Starting digital advertising optimization pipeline...\")\n",
    "    print(f\"Results will be saved to: {run_dir}\")\n",
    "    \n",
    "    # Set seeds for reproducibility\n",
    "    set_all_seeds(42)\n",
    "    \n",
    "    # 1. Generate synthetic data\n",
    "    print(f\"Generating synthetic dataset with {num_samples} samples...\")\n",
    "    dataset = generate_synthetic_data(num_samples)\n",
    "    \n",
    "    # Save dataset\n",
    "    dataset.to_csv(f\"{run_dir}/synthetic_ad_data.csv\", index=False)\n",
    "    print(f\"Synthetic dataset saved to {run_dir}/synthetic_ad_data.csv\")\n",
    "    \n",
    "    # 2. Train RL agent\n",
    "    print(f\"Training RL agent for {train_iterations} iterations...\")\n",
    "    policy, training_metrics, env = train_ad_optimization_agent(\n",
    "        dataset, \n",
    "        num_iterations=train_iterations, \n",
    "        batch_size=100, \n",
    "        sample_size=128\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(policy.state_dict(), f\"{run_dir}/ad_optimization_policy.pt\")\n",
    "    print(f\"Trained model saved to {run_dir}/ad_optimization_policy.pt\")\n",
    "    \n",
    "    # 3. Visualize training progress\n",
    "    print(\"Visualizing training progress...\")\n",
    "    visualize_training_progress(training_metrics, output_dir=plot_dir)\n",
    "    \n",
    "    # 4. Evaluate agent\n",
    "    print(f\"Evaluating agent over {eval_episodes} episodes...\")\n",
    "    eval_metrics = evaluate_policy(policy, env, num_episodes=eval_episodes)\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    with open(f\"{run_dir}/evaluation_metrics.txt\", \"w\") as f:\n",
    "        f.write(f\"Average Reward: {eval_metrics['avg_reward']:.4f}\\n\")\n",
    "        f.write(f\"Success Rate: {eval_metrics['success_rate']:.4f}\\n\")\n",
    "        f.write(f\"Action Distribution: Conservative: {eval_metrics['action_distribution'].get(0, 0):.2f}, \" + \n",
    "                f\"Aggressive: {eval_metrics['action_distribution'].get(1, 0):.2f}\\n\")\n",
    "        f.write(f\"Average Conservative Reward: {eval_metrics['avg_conservative_reward']:.4f}\\n\")\n",
    "        f.write(f\"Average Aggressive Reward: {eval_metrics['avg_aggressive_reward']:.4f}\\n\")\n",
    "    \n",
    "    # 5. Visualize evaluation\n",
    "    print(\"Visualizing evaluation results...\")\n",
    "    visualize_evaluation(eval_metrics, env.feature_columns, output_dir=plot_dir)\n",
    "    \n",
    "    # 6. Analyze strategy across scenarios\n",
    "    print(\"Analyzing strategy across different scenarios...\")\n",
    "    strategy_analysis = analyze_ad_strategy(policy, dataset, env.feature_columns, output_dir=plot_dir)\n",
    "    strategy_analysis.to_csv(f\"{run_dir}/strategy_analysis.csv\", index=False)\n",
    "    \n",
    "    # 7. Analyze feature importance\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    importance_df = feature_importance_analysis(policy, env, env.feature_columns, \n",
    "                                              num_samples=1000, output_dir=plot_dir)\n",
    "    importance_df.to_csv(f\"{run_dir}/feature_importance.csv\", index=False)\n",
    "    \n",
    "    print(f\"Pipeline completed successfully. All results saved to {run_dir}\")\n",
    "    \n",
    "    return {\n",
    "        \"policy\": policy,\n",
    "        \"env\": env,\n",
    "        \"dataset\": dataset,\n",
    "        \"training_metrics\": training_metrics,\n",
    "        \"eval_metrics\": eval_metrics,\n",
    "        \"strategy_analysis\": strategy_analysis,\n",
    "        \"feature_importance\": importance_df,\n",
    "        \"output_dir\": run_dir\n",
    "    }\n",
    "\n",
    "\n",
    "# Execute if run as a script\n",
    "if __name__ == \"__main__\":\n",
    "    results = run_complete_ad_optimization_pipeline(\n",
    "        num_samples=5000,\n",
    "        train_iterations=300,\n",
    "        eval_episodes=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d4253-c53c-478a-ace7-5c9aff174a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20804e2b-9167-4157-ae85-73164062cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting digital advertising optimization pipeline...\n",
      "Results will be saved to: ad_optimization_results_20250307_170304\n",
      "Generating synthetic dataset with 5000 samples...\n",
      "Synthetic dataset saved to ad_optimization_results_20250307_170304/synthetic_ad_data.csv\n",
      "Training RL agent for 300 iterations...\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/objectives/common.py:333: UserWarning: The name value_network wasn't part of the annotations (dict_keys([])). Make sure it is present in the definition class.\n",
      "  warnings.warn(\n",
      "/home/edward/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/objectives/common.py:333: UserWarning: The name value_network_params wasn't part of the annotations (dict_keys([])). Make sure it is present in the definition class.\n",
      "  warnings.warn(\n",
      "/home/edward/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/objectives/common.py:333: UserWarning: The name target_value_network_params wasn't part of the annotations (dict_keys([])). Make sure it is present in the definition class.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0/300, Frames: 100, Avg Reward: 0.0000, Epsilon: 0.9000, Time: 0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/edward/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/_td.py:2654: UserWarning: An output with one or more elements was resized since it had shape [100], which does not match the required output shape [100, 14]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at /pytorch/aten/src/ATen/native/Resize.cpp:30.)\n",
      "  new_dest = torch.stack(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [100, 14] cannot be broadcast to indexing result of shape [100]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mrun_complete_ad_optimization_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43meval_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mrun_complete_ad_optimization_pipeline\u001b[39m\u001b[34m(num_samples, train_iterations, eval_episodes, output_dir)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# 2. Train RL agent\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining RL agent for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m policy, training_metrics, env = \u001b[43mtrain_ad_optimization_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[32m     53\u001b[39m torch.save(policy.state_dict(), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/ad_optimization_policy.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mtrain_ad_optimization_agent\u001b[39m\u001b[34m(dataset, num_iterations, batch_size, sample_size, log_interval)\u001b[39m\n\u001b[32m    119\u001b[39m epsilon_values.append(exploration_module.eps.item())\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Store in replay buffer - make a deep copy to avoid reference issues\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43mrb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m batch_losses = []\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rb) > init_rand_steps:\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# Optimization steps\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/data/replay_buffers/replay_buffers.py:661\u001b[39m, in \u001b[36mReplayBuffer.extend\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.zeros((\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m._storage.ndim), dtype=torch.long)\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/data/replay_buffers/replay_buffers.py:629\u001b[39m, in \u001b[36mReplayBuffer._extend\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim_extend > \u001b[32m0\u001b[39m:\n\u001b[32m    628\u001b[39m         data = \u001b[38;5;28mself\u001b[39m._transpose(data)\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m     \u001b[38;5;28mself\u001b[39m._sampler.extend(index)\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/data/replay_buffers/writers.py:206\u001b[39m, in \u001b[36mRoundRobinWriter.extend\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mself\u001b[39m._write_count += batch_size\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Replicate index requires the shape of the storage to be known\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Other than that, a \"flat\" (1d) index is ok to write the data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    207\u001b[39m index = \u001b[38;5;28mself\u001b[39m._replicate_index(index)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._storage._attached_entities_iter():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/torchrl/data/replay_buffers/storages.py:832\u001b[39m, in \u001b[36mTensorStorage.set\u001b[39m\u001b[34m(self, cursor, data, set_cursor)\u001b[39m\n\u001b[32m    830\u001b[39m         \u001b[38;5;28mself\u001b[39m._init(data)\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tensor_collection(data):\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_storage\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m]\u001b[49m = data\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    834\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_tree_map(cursor, data, \u001b[38;5;28mself\u001b[39m._storage)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/_td.py:867\u001b[39m, in \u001b[36mTensorDict.__setitem__\u001b[39m\u001b[34m(self, index, value)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value_key, item \u001b[38;5;129;01min\u001b[39;00m value.items():\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value_key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_at_str\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m subtd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/_td.py:2556\u001b[39m, in \u001b[36mTensorDict._set_at_str\u001b[39m\u001b[34m(self, key, value, idx, validated, non_blocking)\u001b[39m\n\u001b[32m   2554\u001b[39m     tensor_in.copy_(value, non_blocking=non_blocking)\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tensor_out = \u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensor_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tensor_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_out:\n\u001b[32m   2560\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_shared \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_memmap:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/utils.py:760\u001b[39m, in \u001b[36m_set_item\u001b[39m\u001b[34m(tensor, index, value, validated, non_blocking)\u001b[39m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m760\u001b[39m     \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m = value\n\u001b[32m    761\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/_td.py:867\u001b[39m, in \u001b[36mTensorDict.__setitem__\u001b[39m\u001b[34m(self, index, value)\u001b[39m\n\u001b[32m    865\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value_key, item \u001b[38;5;129;01min\u001b[39;00m value.items():\n\u001b[32m    866\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value_key \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_set_at_str\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    871\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m subtd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/_td.py:2556\u001b[39m, in \u001b[36mTensorDict._set_at_str\u001b[39m\u001b[34m(self, key, value, idx, validated, non_blocking)\u001b[39m\n\u001b[32m   2554\u001b[39m     tensor_in.copy_(value, non_blocking=non_blocking)\n\u001b[32m   2555\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2556\u001b[39m     tensor_out = \u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtensor_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnon_blocking\u001b[49m\n\u001b[32m   2558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tensor_in \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor_out:\n\u001b[32m   2560\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_shared \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_memmap:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/advert/lib/python3.13/site-packages/tensordict/utils.py:739\u001b[39m, in \u001b[36m_set_item\u001b[39m\u001b[34m(tensor, index, value, validated, non_blocking)\u001b[39m\n\u001b[32m    737\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     \u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m = value\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, KeyedJaggedTensor):\n",
      "\u001b[31mRuntimeError\u001b[39m: shape mismatch: value tensor of shape [100, 14] cannot be broadcast to indexing result of shape [100]"
     ]
    }
   ],
   "source": [
    "results = run_complete_ad_optimization_pipeline(\n",
    "        num_samples=5000,\n",
    "        train_iterations=300,\n",
    "        eval_episodes=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f4fb2-e06a-4860-ab2f-95d682be8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ad_optimization_agent(dataset, num_iterations=500, batch_size=100, sample_size=128, log_interval=10):\n",
    "    \"\"\"\n",
    "    Train an RL agent for digital ad optimization with fixed tensor structures.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame with advertising metrics\n",
    "        num_iterations: Number of training iterations\n",
    "        batch_size: Number of environment interactions per batch\n",
    "        sample_size: Number of samples from replay buffer per update\n",
    "        log_interval: Interval for logging training progress\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (trained_policy, training_metrics, environment)\n",
    "    \"\"\"\n",
    "    # Initialize Environment\n",
    "    env = AdOptimizationEnv(dataset)\n",
    "    state_dim = env.num_features\n",
    "    action_dim = env.action_spec.n\n",
    "    \n",
    "    # Create neural network value function\n",
    "    value_mlp = MLP(\n",
    "        in_features=state_dim, \n",
    "        out_features=action_dim, \n",
    "        num_cells=[128, 64],\n",
    "        activation_class=nn.ReLU\n",
    "    )\n",
    "    \n",
    "    # Explicit tensor key mapping for value network\n",
    "    value_net = TensorDictModule(\n",
    "        value_mlp, \n",
    "        in_keys=[\"observation\"], \n",
    "        out_keys=[\"action_value\"]\n",
    "    )\n",
    "    \n",
    "    # Create Q-value module with explicit key configuration\n",
    "    # Note: QValueModule requires careful configuration of out_keys parameter\n",
    "    # The error occurs because QValueModule tries to access out_keys[1] when it's not provided\n",
    "    policy = TensorDictSequential(\n",
    "        value_net,\n",
    "        QValueModule(\n",
    "            action_value_key=\"action_value\",\n",
    "            out_keys=[\"action\", \"action_value\", \"chosen_action_value\"],\n",
    "            spec=env.action_spec  # Ensures the policy has an action space\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Explicitly set spec to policy to avoid \"action_space\" retrieval error\n",
    "    policy.spec = env.action_spec  # Fixes the issue\n",
    "    \n",
    "    # Exploration strategy with epsilon annealing\n",
    "    exploration_module = EGreedyModule(\n",
    "        env.action_spec, \n",
    "        annealing_num_steps=10000, \n",
    "        eps_init=0.9, \n",
    "        eps_end=0.05,\n",
    "        action_key=\"action\"\n",
    "    )\n",
    "    \n",
    "    # Complete policy with exploration\n",
    "    policy_explore = TensorDictSequential(policy, exploration_module)\n",
    "    \n",
    "    # Data collection setup\n",
    "    init_rand_steps = 1000\n",
    "    frames_per_batch = batch_size\n",
    "    optim_steps = 5\n",
    "    \n",
    "    # Create data collector\n",
    "    collector = SyncDataCollector(\n",
    "        env,\n",
    "        policy_explore,\n",
    "        frames_per_batch=frames_per_batch,\n",
    "        total_frames=-1,\n",
    "        init_random_frames=init_rand_steps,\n",
    "        device=\"cpu\",\n",
    "        storing_device=\"cpu\",\n",
    "        return_same_td=True,\n",
    "        split_trajs=False\n",
    "    )\n",
    "    \n",
    "    # Create replay buffer for experience replay\n",
    "    rb = ReplayBuffer(storage=LazyTensorStorage(max_size=20000))\n",
    "    \n",
    "    # Use DQNLoss with standard parameter interface\n",
    "    loss = RobustDQNLoss(\n",
    "        value_network=policy, \n",
    "        action_space=env.action_spec, \n",
    "        delay_value=True\n",
    "    )\n",
    "    \n",
    "    # Optimizer and target network updater\n",
    "    optim = Adam(loss.parameters(), lr=0.001)\n",
    "    updater = SoftUpdate(loss, eps=0.95)\n",
    "    \n",
    "    # Training metrics tracking\n",
    "    total_frames = 0\n",
    "    rewards = []\n",
    "    avg_losses = []\n",
    "    epsilon_values = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training loop\n",
    "    for i, data in enumerate(collector):\n",
    "        if i >= num_iterations:\n",
    "            break\n",
    "            \n",
    "        # Validate and fix tensor dictionary structure\n",
    "        ensure_valid_td_structure(data)\n",
    "        \n",
    "        # Track rewards\n",
    "        if \"reward\" in data:\n",
    "            reward = data[\"reward\"].sum().item() / frames_per_batch\n",
    "            rewards.append(reward)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "        \n",
    "        # Track exploration rate\n",
    "        epsilon_values.append(exploration_module.eps.item())\n",
    "        \n",
    "        # Store in replay buffer - make a deep copy to avoid reference issues\n",
    "        rb.extend(data.clone())\n",
    "        \n",
    "        batch_losses = []\n",
    "        if len(rb) > init_rand_steps:\n",
    "            # Optimization steps\n",
    "            for _ in range(optim_steps):\n",
    "                # Sample from replay buffer\n",
    "                sample = rb.sample(sample_size)\n",
    "                \n",
    "                # Ensure proper structure in the sample\n",
    "                ensure_valid_td_structure(sample)\n",
    "                \n",
    "                # Compute loss and update\n",
    "                loss_vals = loss(sample)\n",
    "                \n",
    "                if \"loss\" in loss_vals:\n",
    "                    loss_val = loss_vals[\"loss\"].item()\n",
    "                    batch_losses.append(loss_val)\n",
    "                    \n",
    "                    loss_vals[\"loss\"].backward()\n",
    "                    optim.step()\n",
    "                    optim.zero_grad()\n",
    "                else:\n",
    "                    print(\"Warning: Loss computation did not return 'loss' key\")\n",
    "                \n",
    "                # Update exploration and target network\n",
    "                exploration_module.step(frames_per_batch)\n",
    "                updater.step()\n",
    "            \n",
    "            if batch_losses:\n",
    "                avg_losses.append(np.mean(batch_losses))\n",
    "        \n",
    "        total_frames += frames_per_batch\n",
    "        \n",
    "        # Print progress\n",
    "        if i % log_interval == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Iteration: {i}/{num_iterations}, Frames: {total_frames}, \"\n",
    "                  f\"Avg Reward: {np.mean(rewards[-10:]) if rewards else 0:.4f}, \"\n",
    "                  f\"Epsilon: {exploration_module.eps.item():.4f}, \"\n",
    "                  f\"Time: {elapsed:.1f}s\")\n",
    "    \n",
    "    # Create clean policy without exploration for evaluation\n",
    "    eval_policy = TensorDictSequential(value_net, QValueModule(spec=env.action_spec))\n",
    "    eval_policy.load_state_dict(policy.state_dict())\n",
    "    \n",
    "    # Compile training metrics\n",
    "    training_metrics = {\n",
    "        \"rewards\": rewards,\n",
    "        \"losses\": avg_losses,\n",
    "        \"epsilon_values\": epsilon_values,\n",
    "        \"total_frames\": total_frames,\n",
    "        \"training_time\": time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    return eval_policy, training_metrics, env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
